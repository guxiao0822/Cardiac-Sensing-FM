{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Finetune CSFM for Custom Tasks\n",
    "\n",
    "This tutorial notebook demonstrates how to adapt the pretrained **CSFM model** to downstream tasks. \n",
    "\n",
    "Before running this notebook, ensure the following:\n",
    "\n",
    "1. The pretrained model checkpoint (`.pth` file) is saved in:  `../pretrained/<checkpoint_file>.pth`\n",
    "\n",
    "2. A local environment is installed and functional following `../README.md`\n",
    "\n",
    "3. Your dataset has been converted to the required `.h5` format based on the `../datasets/prepare_data.ipynb` file and stored in:  ../datasets/<dataset_name>.h5\n",
    "   "
   ],
   "id": "59855471924144ac"
  },
  {
   "cell_type": "markdown",
   "id": "2fb5beaf",
   "metadata": {},
   "source": [
    "# Set up the available GPUs and helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "95c7f222",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:38:58.172292Z",
     "start_time": "2025-06-05T14:38:56.201681Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "# set the visible gpu\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from network.model import CSFM_model\n",
    "import network.lr_decay as lrd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n",
    "import scipy\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Custom meter class\n",
    "class Meter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.sum += value * n\n",
    "        self.count += n\n",
    "\n",
    "    def average(self):\n",
    "        return self.sum / self.count\n",
    "\n",
    "# Function to get the learning rate schedule\n",
    "def get_lr_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step: int):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return max(\n",
    "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Cosine LR schedule with warmup\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step: int):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        # Cosine decay after warmup\n",
    "        return 0.5 * (1 + np.cos(np.pi * (current_step - num_warmup_steps) / (num_training_steps - num_warmup_steps)))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "dd451d56",
   "metadata": {},
   "source": [
    "# Define the hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c0298b40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:39:02.401962Z",
     "start_time": "2025-06-05T14:39:02.398791Z"
    }
   },
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 16\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 20"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "93735b75",
   "metadata": {},
   "source": [
    "# Prepare Training, Validation, and Test Datasets\n",
    "\n",
    "In this demonstration, we use the **VTaC dataset** (Ventricular Tachycardia Alarm Benchmark) for false VT alarm detection. Each data sample contains:\n",
    "\n",
    "- One 10-s ECG signal (electrocardiogram), sampling rate = 250\n",
    "- One 10-s PPG signal (photoplethysmogram), sampling rate = 250"
   ]
  },
  {
   "cell_type": "code",
   "id": "cc7f7622",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:39:04.093739Z",
     "start_time": "2025-06-05T14:39:04.081807Z"
    }
   },
   "source": [
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, h5_file_path, fs, channels):\n",
    "        self.h5_file_path = h5_file_path\n",
    "        self.h5_file = h5py.File(h5_file_path, 'r')\n",
    "        self.signals = self.h5_file['tracings']\n",
    "        self.labels = self.h5_file['labels']\n",
    "        self.subject_ids = None\n",
    "        self.fs = fs\n",
    "        self.channels = channels\n",
    "        self.retry_wait = 1\n",
    "        self.max_retries = 12\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        retries = 0\n",
    "        while retries < self.max_retries:\n",
    "            try:\n",
    "                signal = self.signals[idx, :, :]\n",
    "\n",
    "                if signal.shape[0] > signal.shape[1]:\n",
    "                    signal = signal.T\n",
    "\n",
    "                signal = scipy.signal.resample(signal, num=int(signal.shape[1] * 250 / self.fs), axis=1)\n",
    "                for i, channel in zip(range(signal.shape[0]), self.channels):\n",
    "                    if channel<12:\n",
    "                        signal[i] = nk.ecg_clean(signal[i], sampling_rate=250)\n",
    "                    else:\n",
    "                        signal[i] = nk.ppg_clean(signal[i], sampling_rate=250)\n",
    "\n",
    "                start_idx = random.randint(0, signal.shape[1] - 2500)\n",
    "                signal_segment = signal[:, start_idx:start_idx + 2500]\n",
    "                epsilon = 1e-8  # Small value to prevent division by zero\n",
    "                mean = np.mean(signal_segment, axis=1, keepdims=True)\n",
    "                std = np.std(signal_segment, axis=1, keepdims=True)\n",
    "                signal_segment = (signal_segment - mean) / (std + epsilon)\n",
    "                target = self.labels[idx].squeeze()\n",
    "\n",
    "                return torch.tensor(signal_segment, dtype=torch.float32), torch.tensor(target, dtype=torch.long)\n",
    "            except OSError as e:\n",
    "                print(f\"Error reading data at index {idx}: {e}. Retrying {retries + 1}/{self.max_retries}\")\n",
    "                retries += 1\n",
    "                time.sleep(self.retry_wait)\n",
    "        signal_segment = torch.randn(1, 2500)\n",
    "        target = np.zeros(self.labels.shape[1])\n",
    "        return torch.tensor(signal_segment, dtype=torch.float32), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "    def close(self):\n",
    "        self.h5_file.close()\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "train_dataset = HDF5Dataset(\"../datasets/vtac_train.h5\", fs=250, channels=[1,12])\n",
    "val_dataset = HDF5Dataset(\"../datasets/vtac_val.h5\", fs=250, channels=[1,12])\n",
    "test_dataset = HDF5Dataset(\"../datasets/vtac_test.h5\", fs=250, channels=[1,12])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "multilabel = True"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "6d29875b",
   "metadata": {},
   "source": [
    "# Model initialization and load the checkpoint"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model_type = 'Tiny'  # Options: 'CSFM-Tiny', 'CSFM-Base', 'CSFM-Large'",
   "id": "6fa794331b99507a"
  },
  {
   "cell_type": "code",
   "id": "61d6c06e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:39:08.136771Z",
     "start_time": "2025-06-05T14:39:07.549428Z"
    }
   },
   "source": [
    "model = CSFM_model(model_type)\n",
    "\n",
    "for i, layer in enumerate(model.transformer.layers):\n",
    "    print(f\"Layer {i} mlp_dim:\", layer[1].net[1].out_features)\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "# Load pretrained weights if available\n",
    "checkpoint_path = '../pretrained/checkpoint.pth'\n",
    "print('load from ', checkpoint_path)\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
    "encoder_state_dict = {k.replace('encoder.', ''): v for k, v in checkpoint.items() if k.startswith('encoder.') and 'mlp_head' not in k}\n",
    "\n",
    "model.load_state_dict(encoder_state_dict, strict=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 mlp_dim: 1024\n",
      "Layer 1 mlp_dim: 1024\n",
      "Layer 2 mlp_dim: 1024\n",
      "Layer 3 mlp_dim: 1024\n",
      "Layer 4 mlp_dim: 1024\n",
      "Layer 5 mlp_dim: 1024\n",
      "load from  ../pretrained/checkpoint.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['mlp_head.weight', 'mlp_head.bias'], unexpected_keys=['dense_scratch.layer1_rn.weight', 'dense_scratch.layer2_rn.weight', 'dense_scratch.layer3_rn.weight', 'dense_scratch.layer4_rn.weight', 'dense_scratch.layer_rn.0.weight', 'dense_scratch.layer_rn.1.weight', 'dense_scratch.layer_rn.2.weight', 'dense_scratch.layer_rn.3.weight', 'dense_scratch.refinenet1.out_conv.weight', 'dense_scratch.refinenet1.out_conv.bias', 'dense_scratch.refinenet1.resConfUnit1.conv1.weight', 'dense_scratch.refinenet1.resConfUnit1.conv1.bias', 'dense_scratch.refinenet1.resConfUnit1.conv2.weight', 'dense_scratch.refinenet1.resConfUnit1.conv2.bias', 'dense_scratch.refinenet1.resConfUnit2.conv1.weight', 'dense_scratch.refinenet1.resConfUnit2.conv1.bias', 'dense_scratch.refinenet1.resConfUnit2.conv2.weight', 'dense_scratch.refinenet1.resConfUnit2.conv2.bias', 'dense_scratch.refinenet2.out_conv.weight', 'dense_scratch.refinenet2.out_conv.bias', 'dense_scratch.refinenet2.resConfUnit1.conv1.weight', 'dense_scratch.refinenet2.resConfUnit1.conv1.bias', 'dense_scratch.refinenet2.resConfUnit1.conv2.weight', 'dense_scratch.refinenet2.resConfUnit1.conv2.bias', 'dense_scratch.refinenet2.resConfUnit2.conv1.weight', 'dense_scratch.refinenet2.resConfUnit2.conv1.bias', 'dense_scratch.refinenet2.resConfUnit2.conv2.weight', 'dense_scratch.refinenet2.resConfUnit2.conv2.bias', 'dense_scratch.refinenet3.out_conv.weight', 'dense_scratch.refinenet3.out_conv.bias', 'dense_scratch.refinenet3.resConfUnit1.conv1.weight', 'dense_scratch.refinenet3.resConfUnit1.conv1.bias', 'dense_scratch.refinenet3.resConfUnit1.conv2.weight', 'dense_scratch.refinenet3.resConfUnit1.conv2.bias', 'dense_scratch.refinenet3.resConfUnit2.conv1.weight', 'dense_scratch.refinenet3.resConfUnit2.conv1.bias', 'dense_scratch.refinenet3.resConfUnit2.conv2.weight', 'dense_scratch.refinenet3.resConfUnit2.conv2.bias', 'dense_scratch.refinenet4.out_conv.weight', 'dense_scratch.refinenet4.out_conv.bias', 'dense_scratch.refinenet4.resConfUnit1.conv1.weight', 'dense_scratch.refinenet4.resConfUnit1.conv1.bias', 'dense_scratch.refinenet4.resConfUnit1.conv2.weight', 'dense_scratch.refinenet4.resConfUnit1.conv2.bias', 'dense_scratch.refinenet4.resConfUnit2.conv1.weight', 'dense_scratch.refinenet4.resConfUnit2.conv1.bias', 'dense_scratch.refinenet4.resConfUnit2.conv2.weight', 'dense_scratch.refinenet4.resConfUnit2.conv2.bias', 'act_1_postprocess.0.weight', 'act_1_postprocess.0.bias', 'act_1_postprocess.1.weight', 'act_1_postprocess.1.bias', 'act_2_postprocess.0.weight', 'act_2_postprocess.0.bias', 'act_2_postprocess.1.weight', 'act_2_postprocess.1.bias', 'act_3_postprocess.0.weight', 'act_3_postprocess.0.bias', 'act_4_postprocess.0.weight', 'act_4_postprocess.0.bias', 'act_4_postprocess.1.weight', 'act_4_postprocess.1.bias', 'dense_act_postprocess.0.0.weight', 'dense_act_postprocess.0.0.bias', 'dense_act_postprocess.0.1.weight', 'dense_act_postprocess.0.1.bias', 'dense_act_postprocess.1.0.weight', 'dense_act_postprocess.1.0.bias', 'dense_act_postprocess.1.1.weight', 'dense_act_postprocess.1.1.bias', 'dense_act_postprocess.2.0.weight', 'dense_act_postprocess.2.0.bias', 'dense_act_postprocess.3.0.weight', 'dense_act_postprocess.3.0.bias', 'dense_act_postprocess.3.1.weight', 'dense_act_postprocess.3.1.bias', 'dense_head.0.weight', 'dense_head.0.bias', 'dense_head.2.weight', 'dense_head.2.bias', 'dense_head.4.weight', 'dense_head.4.bias'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "b9ee2491",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "id": "75559501",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:39:10.237444Z",
     "start_time": "2025-06-05T14:39:10.230213Z"
    }
   },
   "source": [
    "# Optimizer and loss function\n",
    "\n",
    "# ASL loss function\n",
    "class ASL(nn.Module):\n",
    "    ''' Notice - optimized version, minimizes memory allocation and gpu uploading,\n",
    "    favors inplace operations'''\n",
    "\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=False, **kwargs):\n",
    "        super(ASL, self).__init__()\n",
    "\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
    "        self.eps = eps\n",
    "\n",
    "        # prevent memory allocation and gpu uploading every iteration, and encourages inplace operations\n",
    "        self.targets = self.anti_targets = self.xs_pos = self.xs_neg = self.asymmetric_w = self.loss = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: input prob (after sigmoid)\n",
    "        y: targets (multi-label binarized vector)\n",
    "        \"\"\"\n",
    "\n",
    "        self.targets = y\n",
    "        self.anti_targets = 1 - y\n",
    "\n",
    "        # Calculating Probabilities\n",
    "        self.xs_pos = x\n",
    "        self.xs_neg = 1.0 - self.xs_pos\n",
    "\n",
    "        # Asymmetric Clipping\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            self.xs_neg.add_(self.clip).clamp_(max=1)\n",
    "\n",
    "        # Basic CE calculation\n",
    "        self.loss = self.targets * torch.log(self.xs_pos.clamp(min=self.eps))\n",
    "        self.loss.add_(self.anti_targets * torch.log(self.xs_neg.clamp(min=self.eps)))\n",
    "\n",
    "        # Asymmetric Focusing\n",
    "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(False)\n",
    "            self.xs_pos = self.xs_pos * self.targets\n",
    "            self.xs_neg = self.xs_neg * self.anti_targets\n",
    "            self.asymmetric_w = torch.pow(1 - self.xs_pos - self.xs_neg,\n",
    "                                          self.gamma_pos * self.targets + self.gamma_neg * self.anti_targets)\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(True)\n",
    "            self.loss *= self.asymmetric_w\n",
    "\n",
    "        return -self.loss.sum()\n",
    "\n",
    "param_groups = lrd.param_groups_lrd(model,\n",
    "    base_lr=learning_rate,\n",
    "    weight_decay=5e-2,\n",
    "    no_weight_decay_list=model.no_weight_decay(),\n",
    "    layer_decay=0.75\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups)\n",
    "criterion = ASL().cuda()\n",
    "\n",
    "# Total number of training steps (number of batches)\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "# Number of warmup steps\n",
    "num_warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "# Initialize the learning rate scheduler\n",
    "scheduler = get_lr_schedule_with_warmup(optimizer, num_warmup_steps, total_steps)\n",
    "\n",
    "# Custom meter for loss tracking\n",
    "train_loss_meter = Meter()\n",
    "val_loss_meter = Meter()\n",
    "\n",
    "channels = np.asarray(train_dataset.channels)\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "best_model_path = None\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "b2aef7dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:39:13.451013Z",
     "start_time": "2025-06-05T14:39:13.446561Z"
    }
   },
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "# Function to calculate F1 Score for each label\n",
    "def calculate_f1_per_label(predictions, targets, multilabel=False):\n",
    "\n",
    "    if multilabel:\n",
    "        predictions = (predictions > 0.5).astype(int)\n",
    "    else:\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    return f1_score(targets, predictions, average=None)\n",
    "\n",
    "# Function to calculate ROC AUC Score\n",
    "def calculate_auc(predictions, targets, multilabel=False):\n",
    "\n",
    "    if multilabel:\n",
    "        return roc_auc_score(targets, predictions, average='macro')\n",
    "    else:\n",
    "        return roc_auc_score(targets, predictions, multi_class='ovr')\n",
    "    \n",
    "# Function to calculate AUPR Score\n",
    "def calculate_aupr(predictions, targets, multilabel=False):\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(targets, predictions)\n",
    "    aupr = auc(recall, precision)\n",
    "    return aupr"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "b2793a22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:44:06.435395Z",
     "start_time": "2025-06-05T14:39:15.631848Z"
    }
   },
   "source": [
    "# Training loop\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss_meter.reset()\n",
    "\n",
    "    all_train_predictions = []\n",
    "    all_train_targets = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, channels, task='cls').squeeze()\n",
    "        \n",
    "        if multilabel:\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "        else:\n",
    "            outputs = torch.softmax(outputs, dim=-1)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        train_loss_meter.update(loss.item(), inputs.size(0))\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update the scheduler\n",
    "\n",
    "        for i in range(len(outputs)):\n",
    "            all_train_predictions.append(outputs[i].cpu().detach().numpy())\n",
    "            all_train_targets.append(targets[i].cpu().detach().numpy())\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs} Batch {batch_idx + 1}/{len(train_loader)}: Train Loss: {loss.item():.4f}')\n",
    "\n",
    "    train_loss = train_loss_meter.average()\n",
    "    train_predictions = np.array(all_train_predictions)\n",
    "    train_targets = np.array(all_train_targets)\n",
    "    train_f1_per_label = calculate_f1_per_label(train_predictions, train_targets, multilabel=multilabel)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss_meter.reset()\n",
    "    all_val_predictions = []\n",
    "    all_val_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            outputs = model(inputs, channels, task='cls').squeeze()\n",
    "        \n",
    "            if multilabel:\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "            else:\n",
    "                outputs = torch.softmax(outputs, dim=-1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss_meter.update(loss.item(), inputs.size(0))\n",
    "            \n",
    "            for i in range(len(outputs)):\n",
    "                all_val_predictions.append(outputs[i].cpu().detach().numpy())\n",
    "                all_val_targets.append(targets[i].cpu().detach().numpy())\n",
    "\n",
    "    val_loss = val_loss_meter.average()\n",
    "    val_predictions = np.array(all_val_predictions)\n",
    "\n",
    "    val_targets = np.array(all_val_targets)\n",
    "\n",
    "    val_f1_per_label = calculate_f1_per_label(val_predictions, val_targets, multilabel=multilabel)\n",
    "    val_f1_mean = np.mean(val_f1_per_label)\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
    "          f'Train Loss: {train_loss:.4f}, Train F1: {np.mean(train_f1_per_label)}'\n",
    "          f'Val Loss: {val_loss:.4f}, Val F1: {np.mean(val_f1_per_label)}')\n",
    "\n",
    "    if val_f1_mean > best_val_f1:\n",
    "        best_val_f1 = val_f1_mean\n",
    "\n",
    "        if best_model_path is not None:\n",
    "            os.remove(best_model_path)  # Remove the previous best model\n",
    "\n",
    "        best_model_path = os.path.join(f'best_model_epoch{epoch + 1}_valf1_{best_val_f1:.4f}.pth')\n",
    "        best_model_state = model.state_dict()\n",
    "        torch.save(best_model_state, best_model_path)\n",
    "        print(f\"Best model saved to {best_model_path}\")\n",
    "\n",
    "# Save the last model\n",
    "last_model_path = os.path.join(f'last_model_epoch{epoch + 1}.pth')\n",
    "if os.path.exists(last_model_path):\n",
    "    os.remove(last_model_path)  # Remove the previous last model\n",
    "\n",
    "torch.save(model.state_dict(), last_model_path)\n",
    "print(f\"Last model saved to {last_model_path}\")\n",
    "\n",
    "# Load for best model for testing\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "print(f\"Best model loaded from {best_model_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Batch 1/232: Train Loss: 1.0091\n",
      "Epoch 1/20 Batch 11/232: Train Loss: 2.6165\n",
      "Epoch 1/20 Batch 21/232: Train Loss: 2.2977\n",
      "Epoch 1/20 Batch 31/232: Train Loss: 1.6573\n",
      "Epoch 1/20 Batch 41/232: Train Loss: 3.0466\n",
      "Epoch 1/20 Batch 51/232: Train Loss: 2.6906\n",
      "Epoch 1/20 Batch 61/232: Train Loss: 2.0949\n",
      "Epoch 1/20 Batch 71/232: Train Loss: 2.2861\n",
      "Epoch 1/20 Batch 81/232: Train Loss: 2.7015\n",
      "Epoch 1/20 Batch 91/232: Train Loss: 1.1174\n",
      "Epoch 1/20 Batch 101/232: Train Loss: 1.5462\n",
      "Epoch 1/20 Batch 111/232: Train Loss: 1.8139\n",
      "Epoch 1/20 Batch 121/232: Train Loss: 1.5849\n",
      "Epoch 1/20 Batch 131/232: Train Loss: 1.7915\n",
      "Epoch 1/20 Batch 141/232: Train Loss: 1.5755\n",
      "Epoch 1/20 Batch 151/232: Train Loss: 1.9806\n",
      "Epoch 1/20 Batch 161/232: Train Loss: 2.3332\n",
      "Epoch 1/20 Batch 171/232: Train Loss: 0.8863\n",
      "Epoch 1/20 Batch 181/232: Train Loss: 1.6646\n",
      "Epoch 1/20 Batch 191/232: Train Loss: 0.8447\n",
      "Epoch 1/20 Batch 201/232: Train Loss: 0.8230\n",
      "Epoch 1/20 Batch 211/232: Train Loss: 1.5349\n",
      "Epoch 1/20 Batch 221/232: Train Loss: 1.1095\n",
      "Epoch 1/20 Batch 231/232: Train Loss: 1.9029\n",
      "Epoch 1/20, Train Loss: 1.5592, Train F1: 0.36628586140250785Val Loss: 1.1908, Val F1: 0.6039594541242728\n",
      "Best model saved to best_model_epoch1_valf1_0.6040.pth\n",
      "Epoch 2/20 Batch 1/232: Train Loss: 1.0669\n",
      "Epoch 2/20 Batch 11/232: Train Loss: 1.8758\n",
      "Epoch 2/20 Batch 21/232: Train Loss: 1.2209\n",
      "Epoch 2/20 Batch 31/232: Train Loss: 1.9824\n",
      "Epoch 2/20 Batch 41/232: Train Loss: 1.5701\n",
      "Epoch 2/20 Batch 51/232: Train Loss: 1.2778\n",
      "Epoch 2/20 Batch 61/232: Train Loss: 1.2303\n",
      "Epoch 2/20 Batch 71/232: Train Loss: 0.7857\n",
      "Epoch 2/20 Batch 81/232: Train Loss: 1.0271\n",
      "Epoch 2/20 Batch 91/232: Train Loss: 0.8020\n",
      "Epoch 2/20 Batch 101/232: Train Loss: 1.1851\n",
      "Epoch 2/20 Batch 111/232: Train Loss: 0.7359\n",
      "Epoch 2/20 Batch 121/232: Train Loss: 0.5183\n",
      "Epoch 2/20 Batch 131/232: Train Loss: 0.7268\n",
      "Epoch 2/20 Batch 141/232: Train Loss: 1.4399\n",
      "Epoch 2/20 Batch 151/232: Train Loss: 1.6314\n",
      "Epoch 2/20 Batch 161/232: Train Loss: 0.9301\n",
      "Epoch 2/20 Batch 171/232: Train Loss: 0.6621\n",
      "Epoch 2/20 Batch 181/232: Train Loss: 0.7932\n",
      "Epoch 2/20 Batch 191/232: Train Loss: 1.3987\n",
      "Epoch 2/20 Batch 201/232: Train Loss: 1.6574\n",
      "Epoch 2/20 Batch 211/232: Train Loss: 1.3957\n",
      "Epoch 2/20 Batch 221/232: Train Loss: 0.7147\n",
      "Epoch 2/20 Batch 231/232: Train Loss: 0.6794\n",
      "Epoch 2/20, Train Loss: 1.0616, Train F1: 0.7640000015795674Val Loss: 1.0911, Val F1: 0.7121472670743184\n",
      "Best model saved to best_model_epoch2_valf1_0.7121.pth\n",
      "Epoch 3/20 Batch 1/232: Train Loss: 1.7776\n",
      "Epoch 3/20 Batch 11/232: Train Loss: 0.6892\n",
      "Epoch 3/20 Batch 21/232: Train Loss: 0.3502\n",
      "Epoch 3/20 Batch 31/232: Train Loss: 0.9922\n",
      "Epoch 3/20 Batch 41/232: Train Loss: 1.4717\n",
      "Epoch 3/20 Batch 51/232: Train Loss: 0.8969\n",
      "Epoch 3/20 Batch 61/232: Train Loss: 0.3209\n",
      "Epoch 3/20 Batch 71/232: Train Loss: 1.2009\n",
      "Epoch 3/20 Batch 81/232: Train Loss: 1.1913\n",
      "Epoch 3/20 Batch 91/232: Train Loss: 2.2769\n",
      "Epoch 3/20 Batch 101/232: Train Loss: 0.9056\n",
      "Epoch 3/20 Batch 111/232: Train Loss: 0.8502\n",
      "Epoch 3/20 Batch 121/232: Train Loss: 1.0279\n",
      "Epoch 3/20 Batch 131/232: Train Loss: 1.1900\n",
      "Epoch 3/20 Batch 141/232: Train Loss: 0.9679\n",
      "Epoch 3/20 Batch 151/232: Train Loss: 1.1944\n",
      "Epoch 3/20 Batch 161/232: Train Loss: 0.3251\n",
      "Epoch 3/20 Batch 171/232: Train Loss: 1.1096\n",
      "Epoch 3/20 Batch 181/232: Train Loss: 0.9209\n",
      "Epoch 3/20 Batch 191/232: Train Loss: 1.1422\n",
      "Epoch 3/20 Batch 201/232: Train Loss: 1.5576\n",
      "Epoch 3/20 Batch 211/232: Train Loss: 0.9218\n",
      "Epoch 3/20 Batch 221/232: Train Loss: 1.8854\n",
      "Epoch 3/20 Batch 231/232: Train Loss: 0.5802\n",
      "Epoch 3/20, Train Loss: 0.8891, Train F1: 0.8089979241002156Val Loss: 0.9380, Val F1: 0.7773154251812788\n",
      "Best model saved to best_model_epoch3_valf1_0.7773.pth\n",
      "Epoch 4/20 Batch 1/232: Train Loss: 0.5806\n",
      "Epoch 4/20 Batch 11/232: Train Loss: 0.8192\n",
      "Epoch 4/20 Batch 21/232: Train Loss: 0.5069\n",
      "Epoch 4/20 Batch 31/232: Train Loss: 0.3283\n",
      "Epoch 4/20 Batch 41/232: Train Loss: 0.9523\n",
      "Epoch 4/20 Batch 51/232: Train Loss: 0.6755\n",
      "Epoch 4/20 Batch 61/232: Train Loss: 0.3659\n",
      "Epoch 4/20 Batch 71/232: Train Loss: 1.0582\n",
      "Epoch 4/20 Batch 81/232: Train Loss: 1.9344\n",
      "Epoch 4/20 Batch 91/232: Train Loss: 1.1667\n",
      "Epoch 4/20 Batch 101/232: Train Loss: 1.0789\n",
      "Epoch 4/20 Batch 111/232: Train Loss: 0.3446\n",
      "Epoch 4/20 Batch 121/232: Train Loss: 0.8575\n",
      "Epoch 4/20 Batch 131/232: Train Loss: 1.1561\n",
      "Epoch 4/20 Batch 141/232: Train Loss: 0.3026\n",
      "Epoch 4/20 Batch 151/232: Train Loss: 0.6433\n",
      "Epoch 4/20 Batch 161/232: Train Loss: 0.4041\n",
      "Epoch 4/20 Batch 171/232: Train Loss: 0.6731\n",
      "Epoch 4/20 Batch 181/232: Train Loss: 1.4388\n",
      "Epoch 4/20 Batch 191/232: Train Loss: 0.8512\n",
      "Epoch 4/20 Batch 201/232: Train Loss: 1.1745\n",
      "Epoch 4/20 Batch 211/232: Train Loss: 1.3858\n",
      "Epoch 4/20 Batch 221/232: Train Loss: 0.7985\n",
      "Epoch 4/20 Batch 231/232: Train Loss: 0.8280\n",
      "Epoch 4/20, Train Loss: 0.7798, Train F1: 0.8372738162789232Val Loss: 0.8335, Val F1: 0.8406098406098406\n",
      "Best model saved to best_model_epoch4_valf1_0.8406.pth\n",
      "Epoch 5/20 Batch 1/232: Train Loss: 1.0543\n",
      "Epoch 5/20 Batch 11/232: Train Loss: 0.8146\n",
      "Epoch 5/20 Batch 21/232: Train Loss: 0.1969\n",
      "Epoch 5/20 Batch 31/232: Train Loss: 0.6899\n",
      "Epoch 5/20 Batch 41/232: Train Loss: 0.2412\n",
      "Epoch 5/20 Batch 51/232: Train Loss: 0.3751\n",
      "Epoch 5/20 Batch 61/232: Train Loss: 0.3714\n",
      "Epoch 5/20 Batch 71/232: Train Loss: 0.5354\n",
      "Epoch 5/20 Batch 81/232: Train Loss: 0.1303\n",
      "Epoch 5/20 Batch 91/232: Train Loss: 0.5144\n",
      "Epoch 5/20 Batch 101/232: Train Loss: 1.5555\n",
      "Epoch 5/20 Batch 111/232: Train Loss: 0.9842\n",
      "Epoch 5/20 Batch 121/232: Train Loss: 0.7885\n",
      "Epoch 5/20 Batch 131/232: Train Loss: 0.4301\n",
      "Epoch 5/20 Batch 141/232: Train Loss: 0.3742\n",
      "Epoch 5/20 Batch 151/232: Train Loss: 0.4224\n",
      "Epoch 5/20 Batch 161/232: Train Loss: 0.8627\n",
      "Epoch 5/20 Batch 171/232: Train Loss: 0.3464\n",
      "Epoch 5/20 Batch 181/232: Train Loss: 0.2627\n",
      "Epoch 5/20 Batch 191/232: Train Loss: 0.5088\n",
      "Epoch 5/20 Batch 201/232: Train Loss: 0.8453\n",
      "Epoch 5/20 Batch 211/232: Train Loss: 0.1490\n",
      "Epoch 5/20 Batch 221/232: Train Loss: 1.2860\n",
      "Epoch 5/20 Batch 231/232: Train Loss: 0.3919\n",
      "Epoch 5/20, Train Loss: 0.6822, Train F1: 0.8583864118895966Val Loss: 0.9313, Val F1: 0.7951362996105726\n",
      "Epoch 6/20 Batch 1/232: Train Loss: 1.1963\n",
      "Epoch 6/20 Batch 11/232: Train Loss: 0.8373\n",
      "Epoch 6/20 Batch 21/232: Train Loss: 0.9874\n",
      "Epoch 6/20 Batch 31/232: Train Loss: 1.3031\n",
      "Epoch 6/20 Batch 41/232: Train Loss: 0.5039\n",
      "Epoch 6/20 Batch 51/232: Train Loss: 0.4938\n",
      "Epoch 6/20 Batch 61/232: Train Loss: 0.5939\n",
      "Epoch 6/20 Batch 71/232: Train Loss: 0.6827\n",
      "Epoch 6/20 Batch 81/232: Train Loss: 0.5303\n",
      "Epoch 6/20 Batch 91/232: Train Loss: 0.4074\n",
      "Epoch 6/20 Batch 101/232: Train Loss: 1.1222\n",
      "Epoch 6/20 Batch 111/232: Train Loss: 1.1252\n",
      "Epoch 6/20 Batch 121/232: Train Loss: 0.2002\n",
      "Epoch 6/20 Batch 131/232: Train Loss: 0.1890\n",
      "Epoch 6/20 Batch 141/232: Train Loss: 1.3258\n",
      "Epoch 6/20 Batch 151/232: Train Loss: 0.7554\n",
      "Epoch 6/20 Batch 161/232: Train Loss: 0.3195\n",
      "Epoch 6/20 Batch 171/232: Train Loss: 0.1862\n",
      "Epoch 6/20 Batch 181/232: Train Loss: 1.6949\n",
      "Epoch 6/20 Batch 191/232: Train Loss: 0.9401\n",
      "Epoch 6/20 Batch 201/232: Train Loss: 0.2746\n",
      "Epoch 6/20 Batch 211/232: Train Loss: 1.0174\n",
      "Epoch 6/20 Batch 221/232: Train Loss: 0.1351\n",
      "Epoch 6/20 Batch 231/232: Train Loss: 0.5132\n",
      "Epoch 6/20, Train Loss: 0.6211, Train F1: 0.8729356014875713Val Loss: 0.9816, Val F1: 0.8286176577036894\n",
      "Epoch 7/20 Batch 1/232: Train Loss: 0.2449\n",
      "Epoch 7/20 Batch 11/232: Train Loss: 1.0815\n",
      "Epoch 7/20 Batch 21/232: Train Loss: 0.8335\n",
      "Epoch 7/20 Batch 31/232: Train Loss: 0.2231\n",
      "Epoch 7/20 Batch 41/232: Train Loss: 1.6942\n",
      "Epoch 7/20 Batch 51/232: Train Loss: 0.8559\n",
      "Epoch 7/20 Batch 61/232: Train Loss: 0.2816\n",
      "Epoch 7/20 Batch 71/232: Train Loss: 0.5162\n",
      "Epoch 7/20 Batch 81/232: Train Loss: 0.6832\n",
      "Epoch 7/20 Batch 91/232: Train Loss: 0.7254\n",
      "Epoch 7/20 Batch 101/232: Train Loss: 0.1178\n",
      "Epoch 7/20 Batch 111/232: Train Loss: 0.5351\n",
      "Epoch 7/20 Batch 121/232: Train Loss: 1.7590\n",
      "Epoch 7/20 Batch 131/232: Train Loss: 0.7772\n",
      "Epoch 7/20 Batch 141/232: Train Loss: 0.1447\n",
      "Epoch 7/20 Batch 151/232: Train Loss: 0.6211\n",
      "Epoch 7/20 Batch 161/232: Train Loss: 0.3790\n",
      "Epoch 7/20 Batch 171/232: Train Loss: 0.1572\n",
      "Epoch 7/20 Batch 181/232: Train Loss: 0.9541\n",
      "Epoch 7/20 Batch 191/232: Train Loss: 0.5157\n",
      "Epoch 7/20 Batch 201/232: Train Loss: 0.5689\n",
      "Epoch 7/20 Batch 211/232: Train Loss: 0.3700\n",
      "Epoch 7/20 Batch 221/232: Train Loss: 1.1031\n",
      "Epoch 7/20 Batch 231/232: Train Loss: 1.1255\n",
      "Epoch 7/20, Train Loss: 0.5715, Train F1: 0.8853551115623697Val Loss: 1.0882, Val F1: 0.8035416666666666\n",
      "Epoch 8/20 Batch 1/232: Train Loss: 0.1603\n",
      "Epoch 8/20 Batch 11/232: Train Loss: 0.1729\n",
      "Epoch 8/20 Batch 21/232: Train Loss: 1.0072\n",
      "Epoch 8/20 Batch 31/232: Train Loss: 1.1296\n",
      "Epoch 8/20 Batch 41/232: Train Loss: 0.5450\n",
      "Epoch 8/20 Batch 51/232: Train Loss: 0.2793\n",
      "Epoch 8/20 Batch 61/232: Train Loss: 0.7845\n",
      "Epoch 8/20 Batch 71/232: Train Loss: 0.2347\n",
      "Epoch 8/20 Batch 81/232: Train Loss: 0.1445\n",
      "Epoch 8/20 Batch 91/232: Train Loss: 0.4960\n",
      "Epoch 8/20 Batch 101/232: Train Loss: 0.5601\n",
      "Epoch 8/20 Batch 111/232: Train Loss: 0.4202\n",
      "Epoch 8/20 Batch 121/232: Train Loss: 0.4208\n",
      "Epoch 8/20 Batch 131/232: Train Loss: 0.3778\n",
      "Epoch 8/20 Batch 141/232: Train Loss: 0.5541\n",
      "Epoch 8/20 Batch 151/232: Train Loss: 0.8366\n",
      "Epoch 8/20 Batch 161/232: Train Loss: 1.1347\n",
      "Epoch 8/20 Batch 171/232: Train Loss: 1.3631\n",
      "Epoch 8/20 Batch 181/232: Train Loss: 0.8840\n",
      "Epoch 8/20 Batch 191/232: Train Loss: 1.7283\n",
      "Epoch 8/20 Batch 201/232: Train Loss: 0.2658\n",
      "Epoch 8/20 Batch 211/232: Train Loss: 0.7151\n",
      "Epoch 8/20 Batch 221/232: Train Loss: 0.3741\n",
      "Epoch 8/20 Batch 231/232: Train Loss: 0.2407\n",
      "Epoch 8/20, Train Loss: 0.5258, Train F1: 0.8901489867519836Val Loss: 0.8286, Val F1: 0.8716517857142857\n",
      "Best model saved to best_model_epoch8_valf1_0.8717.pth\n",
      "Epoch 9/20 Batch 1/232: Train Loss: 0.3040\n",
      "Epoch 9/20 Batch 11/232: Train Loss: 0.0883\n",
      "Epoch 9/20 Batch 21/232: Train Loss: 0.2384\n",
      "Epoch 9/20 Batch 31/232: Train Loss: 0.1141\n",
      "Epoch 9/20 Batch 41/232: Train Loss: 0.3347\n",
      "Epoch 9/20 Batch 51/232: Train Loss: 0.1732\n",
      "Epoch 9/20 Batch 61/232: Train Loss: 0.3241\n",
      "Epoch 9/20 Batch 71/232: Train Loss: 0.1226\n",
      "Epoch 9/20 Batch 81/232: Train Loss: 0.4325\n",
      "Epoch 9/20 Batch 91/232: Train Loss: 0.1846\n",
      "Epoch 9/20 Batch 101/232: Train Loss: 0.2305\n",
      "Epoch 9/20 Batch 111/232: Train Loss: 0.2354\n",
      "Epoch 9/20 Batch 121/232: Train Loss: 0.8626\n",
      "Epoch 9/20 Batch 131/232: Train Loss: 1.5104\n",
      "Epoch 9/20 Batch 141/232: Train Loss: 0.0656\n",
      "Epoch 9/20 Batch 151/232: Train Loss: 0.7813\n",
      "Epoch 9/20 Batch 161/232: Train Loss: 0.5554\n",
      "Epoch 9/20 Batch 171/232: Train Loss: 0.3955\n",
      "Epoch 9/20 Batch 181/232: Train Loss: 0.1080\n",
      "Epoch 9/20 Batch 191/232: Train Loss: 0.7365\n",
      "Epoch 9/20 Batch 201/232: Train Loss: 0.1107\n",
      "Epoch 9/20 Batch 211/232: Train Loss: 0.8095\n",
      "Epoch 9/20 Batch 221/232: Train Loss: 0.5712\n",
      "Epoch 9/20 Batch 231/232: Train Loss: 0.8060\n",
      "Epoch 9/20, Train Loss: 0.4374, Train F1: 0.9176929381156942Val Loss: 0.9303, Val F1: 0.8461339260665961\n",
      "Epoch 10/20 Batch 1/232: Train Loss: 0.3849\n",
      "Epoch 10/20 Batch 11/232: Train Loss: 0.2612\n",
      "Epoch 10/20 Batch 21/232: Train Loss: 0.4207\n",
      "Epoch 10/20 Batch 31/232: Train Loss: 0.1341\n",
      "Epoch 10/20 Batch 41/232: Train Loss: 0.7831\n",
      "Epoch 10/20 Batch 51/232: Train Loss: 0.1447\n",
      "Epoch 10/20 Batch 61/232: Train Loss: 0.0828\n",
      "Epoch 10/20 Batch 71/232: Train Loss: 0.1314\n",
      "Epoch 10/20 Batch 81/232: Train Loss: 0.0383\n",
      "Epoch 10/20 Batch 91/232: Train Loss: 0.7805\n",
      "Epoch 10/20 Batch 101/232: Train Loss: 0.2124\n",
      "Epoch 10/20 Batch 111/232: Train Loss: 0.5182\n",
      "Epoch 10/20 Batch 121/232: Train Loss: 0.4254\n",
      "Epoch 10/20 Batch 131/232: Train Loss: 0.2491\n",
      "Epoch 10/20 Batch 141/232: Train Loss: 0.2564\n",
      "Epoch 10/20 Batch 151/232: Train Loss: 0.5115\n",
      "Epoch 10/20 Batch 161/232: Train Loss: 0.4162\n",
      "Epoch 10/20 Batch 171/232: Train Loss: 0.3082\n",
      "Epoch 10/20 Batch 181/232: Train Loss: 0.1689\n",
      "Epoch 10/20 Batch 191/232: Train Loss: 0.0761\n",
      "Epoch 10/20 Batch 201/232: Train Loss: 0.3191\n",
      "Epoch 10/20 Batch 211/232: Train Loss: 0.1116\n",
      "Epoch 10/20 Batch 221/232: Train Loss: 0.6822\n",
      "Epoch 10/20 Batch 231/232: Train Loss: 0.3516\n",
      "Epoch 10/20, Train Loss: 0.3860, Train F1: 0.9237915650655553Val Loss: 1.0215, Val F1: 0.8567518397718519\n",
      "Epoch 11/20 Batch 1/232: Train Loss: 0.1120\n",
      "Epoch 11/20 Batch 11/232: Train Loss: 0.1432\n",
      "Epoch 11/20 Batch 21/232: Train Loss: 0.1040\n",
      "Epoch 11/20 Batch 31/232: Train Loss: 0.3261\n",
      "Epoch 11/20 Batch 41/232: Train Loss: 0.3898\n",
      "Epoch 11/20 Batch 51/232: Train Loss: 0.5310\n",
      "Epoch 11/20 Batch 61/232: Train Loss: 0.8896\n",
      "Epoch 11/20 Batch 71/232: Train Loss: 0.3545\n",
      "Epoch 11/20 Batch 81/232: Train Loss: 0.1227\n",
      "Epoch 11/20 Batch 91/232: Train Loss: 0.2301\n",
      "Epoch 11/20 Batch 101/232: Train Loss: 0.0978\n",
      "Epoch 11/20 Batch 111/232: Train Loss: 0.2735\n",
      "Epoch 11/20 Batch 121/232: Train Loss: 1.1439\n",
      "Epoch 11/20 Batch 131/232: Train Loss: 0.1144\n",
      "Epoch 11/20 Batch 141/232: Train Loss: 0.1631\n",
      "Epoch 11/20 Batch 151/232: Train Loss: 0.3488\n",
      "Epoch 11/20 Batch 161/232: Train Loss: 0.0662\n",
      "Epoch 11/20 Batch 171/232: Train Loss: 0.1430\n",
      "Epoch 11/20 Batch 181/232: Train Loss: 0.1777\n",
      "Epoch 11/20 Batch 191/232: Train Loss: 0.1751\n",
      "Epoch 11/20 Batch 201/232: Train Loss: 0.2701\n",
      "Epoch 11/20 Batch 211/232: Train Loss: 0.1437\n",
      "Epoch 11/20 Batch 221/232: Train Loss: 0.6586\n",
      "Epoch 11/20 Batch 231/232: Train Loss: 0.1338\n",
      "Epoch 11/20, Train Loss: 0.3663, Train F1: 0.9280688605518211Val Loss: 0.9995, Val F1: 0.8394555448913708\n",
      "Epoch 12/20 Batch 1/232: Train Loss: 0.2616\n",
      "Epoch 12/20 Batch 11/232: Train Loss: 0.5927\n",
      "Epoch 12/20 Batch 21/232: Train Loss: 0.8201\n",
      "Epoch 12/20 Batch 31/232: Train Loss: 0.0753\n",
      "Epoch 12/20 Batch 41/232: Train Loss: 0.0840\n",
      "Epoch 12/20 Batch 51/232: Train Loss: 0.6599\n",
      "Epoch 12/20 Batch 61/232: Train Loss: 0.0521\n",
      "Epoch 12/20 Batch 71/232: Train Loss: 0.0444\n",
      "Epoch 12/20 Batch 81/232: Train Loss: 0.2062\n",
      "Epoch 12/20 Batch 91/232: Train Loss: 0.1138\n",
      "Epoch 12/20 Batch 101/232: Train Loss: 0.2553\n",
      "Epoch 12/20 Batch 111/232: Train Loss: 0.0353\n",
      "Epoch 12/20 Batch 121/232: Train Loss: 0.5054\n",
      "Epoch 12/20 Batch 131/232: Train Loss: 0.0855\n",
      "Epoch 12/20 Batch 141/232: Train Loss: 0.0373\n",
      "Epoch 12/20 Batch 151/232: Train Loss: 0.0313\n",
      "Epoch 12/20 Batch 161/232: Train Loss: 1.1758\n",
      "Epoch 12/20 Batch 171/232: Train Loss: 0.4769\n",
      "Epoch 12/20 Batch 181/232: Train Loss: 0.0731\n",
      "Epoch 12/20 Batch 191/232: Train Loss: 0.1979\n",
      "Epoch 12/20 Batch 201/232: Train Loss: 0.7132\n",
      "Epoch 12/20 Batch 211/232: Train Loss: 0.3429\n",
      "Epoch 12/20 Batch 221/232: Train Loss: 0.1139\n",
      "Epoch 12/20 Batch 231/232: Train Loss: 0.4155\n",
      "Epoch 12/20, Train Loss: 0.2900, Train F1: 0.9430080201998667Val Loss: 1.1170, Val F1: 0.8647591681412984\n",
      "Epoch 13/20 Batch 1/232: Train Loss: 0.1008\n",
      "Epoch 13/20 Batch 11/232: Train Loss: 0.7787\n",
      "Epoch 13/20 Batch 21/232: Train Loss: 0.2224\n",
      "Epoch 13/20 Batch 31/232: Train Loss: 0.0460\n",
      "Epoch 13/20 Batch 41/232: Train Loss: 0.0473\n",
      "Epoch 13/20 Batch 51/232: Train Loss: 0.1507\n",
      "Epoch 13/20 Batch 61/232: Train Loss: 0.0540\n",
      "Epoch 13/20 Batch 71/232: Train Loss: 0.0810\n",
      "Epoch 13/20 Batch 81/232: Train Loss: 0.0294\n",
      "Epoch 13/20 Batch 91/232: Train Loss: 0.7656\n",
      "Epoch 13/20 Batch 101/232: Train Loss: 0.0512\n",
      "Epoch 13/20 Batch 111/232: Train Loss: 0.0480\n",
      "Epoch 13/20 Batch 121/232: Train Loss: 0.6041\n",
      "Epoch 13/20 Batch 131/232: Train Loss: 0.2252\n",
      "Epoch 13/20 Batch 141/232: Train Loss: 0.0556\n",
      "Epoch 13/20 Batch 151/232: Train Loss: 0.4265\n",
      "Epoch 13/20 Batch 161/232: Train Loss: 0.0360\n",
      "Epoch 13/20 Batch 171/232: Train Loss: 0.0404\n",
      "Epoch 13/20 Batch 181/232: Train Loss: 0.0417\n",
      "Epoch 13/20 Batch 191/232: Train Loss: 0.4436\n",
      "Epoch 13/20 Batch 201/232: Train Loss: 0.0872\n",
      "Epoch 13/20 Batch 211/232: Train Loss: 0.0405\n",
      "Epoch 13/20 Batch 221/232: Train Loss: 0.0412\n",
      "Epoch 13/20 Batch 231/232: Train Loss: 0.6398\n",
      "Epoch 13/20, Train Loss: 0.2589, Train F1: 0.9594299745077827Val Loss: 1.1215, Val F1: 0.8981351981351982\n",
      "Best model saved to best_model_epoch13_valf1_0.8981.pth\n",
      "Epoch 14/20 Batch 1/232: Train Loss: 1.3276\n",
      "Epoch 14/20 Batch 11/232: Train Loss: 0.0714\n",
      "Epoch 14/20 Batch 21/232: Train Loss: 0.0704\n",
      "Epoch 14/20 Batch 31/232: Train Loss: 0.2543\n",
      "Epoch 14/20 Batch 41/232: Train Loss: 0.7689\n",
      "Epoch 14/20 Batch 51/232: Train Loss: 1.0912\n",
      "Epoch 14/20 Batch 61/232: Train Loss: 0.0911\n",
      "Epoch 14/20 Batch 71/232: Train Loss: 0.4295\n",
      "Epoch 14/20 Batch 81/232: Train Loss: 0.0187\n",
      "Epoch 14/20 Batch 91/232: Train Loss: 0.9949\n",
      "Epoch 14/20 Batch 101/232: Train Loss: 0.3735\n",
      "Epoch 14/20 Batch 111/232: Train Loss: 0.0509\n",
      "Epoch 14/20 Batch 121/232: Train Loss: 0.0902\n",
      "Epoch 14/20 Batch 131/232: Train Loss: 0.2840\n",
      "Epoch 14/20 Batch 141/232: Train Loss: 0.1475\n",
      "Epoch 14/20 Batch 151/232: Train Loss: 0.0451\n",
      "Epoch 14/20 Batch 161/232: Train Loss: 0.0936\n",
      "Epoch 14/20 Batch 171/232: Train Loss: 0.2938\n",
      "Epoch 14/20 Batch 181/232: Train Loss: 0.3770\n",
      "Epoch 14/20 Batch 191/232: Train Loss: 0.0357\n",
      "Epoch 14/20 Batch 201/232: Train Loss: 0.0573\n",
      "Epoch 14/20 Batch 211/232: Train Loss: 0.1349\n",
      "Epoch 14/20 Batch 221/232: Train Loss: 0.3154\n",
      "Epoch 14/20 Batch 231/232: Train Loss: 0.0733\n",
      "Epoch 14/20, Train Loss: 0.2254, Train F1: 0.9550893256519322Val Loss: 1.1591, Val F1: 0.8889425398358282\n",
      "Epoch 15/20 Batch 1/232: Train Loss: 0.0598\n",
      "Epoch 15/20 Batch 11/232: Train Loss: 0.3114\n",
      "Epoch 15/20 Batch 21/232: Train Loss: 0.3324\n",
      "Epoch 15/20 Batch 31/232: Train Loss: 0.7032\n",
      "Epoch 15/20 Batch 41/232: Train Loss: 0.0769\n",
      "Epoch 15/20 Batch 51/232: Train Loss: 0.4298\n",
      "Epoch 15/20 Batch 61/232: Train Loss: 0.0566\n",
      "Epoch 15/20 Batch 71/232: Train Loss: 0.2718\n",
      "Epoch 15/20 Batch 81/232: Train Loss: 0.0445\n",
      "Epoch 15/20 Batch 91/232: Train Loss: 0.0561\n",
      "Epoch 15/20 Batch 101/232: Train Loss: 0.1307\n",
      "Epoch 15/20 Batch 111/232: Train Loss: 0.0434\n",
      "Epoch 15/20 Batch 121/232: Train Loss: 0.2289\n",
      "Epoch 15/20 Batch 131/232: Train Loss: 0.3093\n",
      "Epoch 15/20 Batch 141/232: Train Loss: 0.2898\n",
      "Epoch 15/20 Batch 151/232: Train Loss: 0.0451\n",
      "Epoch 15/20 Batch 161/232: Train Loss: 0.0420\n",
      "Epoch 15/20 Batch 171/232: Train Loss: 0.1855\n",
      "Epoch 15/20 Batch 181/232: Train Loss: 0.1814\n",
      "Epoch 15/20 Batch 191/232: Train Loss: 0.0395\n",
      "Epoch 15/20 Batch 201/232: Train Loss: 0.1057\n",
      "Epoch 15/20 Batch 211/232: Train Loss: 0.0295\n",
      "Epoch 15/20 Batch 221/232: Train Loss: 0.0593\n",
      "Epoch 15/20 Batch 231/232: Train Loss: 0.1720\n",
      "Epoch 15/20, Train Loss: 0.2359, Train F1: 0.955720429721778Val Loss: 1.1542, Val F1: 0.8823054486322983\n",
      "Epoch 16/20 Batch 1/232: Train Loss: 0.2698\n",
      "Epoch 16/20 Batch 11/232: Train Loss: 0.0910\n",
      "Epoch 16/20 Batch 21/232: Train Loss: 0.1856\n",
      "Epoch 16/20 Batch 31/232: Train Loss: 0.9948\n",
      "Epoch 16/20 Batch 41/232: Train Loss: 0.0539\n",
      "Epoch 16/20 Batch 51/232: Train Loss: 0.0725\n",
      "Epoch 16/20 Batch 61/232: Train Loss: 0.1138\n",
      "Epoch 16/20 Batch 71/232: Train Loss: 0.0171\n",
      "Epoch 16/20 Batch 81/232: Train Loss: 0.0850\n",
      "Epoch 16/20 Batch 91/232: Train Loss: 0.1519\n",
      "Epoch 16/20 Batch 101/232: Train Loss: 0.0851\n",
      "Epoch 16/20 Batch 111/232: Train Loss: 0.1248\n",
      "Epoch 16/20 Batch 121/232: Train Loss: 0.0493\n",
      "Epoch 16/20 Batch 131/232: Train Loss: 0.0769\n",
      "Epoch 16/20 Batch 141/232: Train Loss: 0.1777\n",
      "Epoch 16/20 Batch 151/232: Train Loss: 0.0357\n",
      "Epoch 16/20 Batch 161/232: Train Loss: 0.0540\n",
      "Epoch 16/20 Batch 171/232: Train Loss: 0.0250\n",
      "Epoch 16/20 Batch 181/232: Train Loss: 1.4283\n",
      "Epoch 16/20 Batch 191/232: Train Loss: 0.4743\n",
      "Epoch 16/20 Batch 201/232: Train Loss: 0.0734\n",
      "Epoch 16/20 Batch 211/232: Train Loss: 0.0264\n",
      "Epoch 16/20 Batch 221/232: Train Loss: 0.0963\n",
      "Epoch 16/20 Batch 231/232: Train Loss: 0.0478\n",
      "Epoch 16/20, Train Loss: 0.1705, Train F1: 0.9662705984852129Val Loss: 1.3021, Val F1: 0.8781113460183227\n",
      "Epoch 17/20 Batch 1/232: Train Loss: 0.0368\n",
      "Epoch 17/20 Batch 11/232: Train Loss: 0.0400\n",
      "Epoch 17/20 Batch 21/232: Train Loss: 0.2447\n",
      "Epoch 17/20 Batch 31/232: Train Loss: 0.2666\n",
      "Epoch 17/20 Batch 41/232: Train Loss: 0.1891\n",
      "Epoch 17/20 Batch 51/232: Train Loss: 0.0442\n",
      "Epoch 17/20 Batch 61/232: Train Loss: 0.2648\n",
      "Epoch 17/20 Batch 71/232: Train Loss: 0.0271\n",
      "Epoch 17/20 Batch 81/232: Train Loss: 0.0541\n",
      "Epoch 17/20 Batch 91/232: Train Loss: 0.1705\n",
      "Epoch 17/20 Batch 101/232: Train Loss: 0.0819\n",
      "Epoch 17/20 Batch 111/232: Train Loss: 0.1597\n",
      "Epoch 17/20 Batch 121/232: Train Loss: 0.0456\n",
      "Epoch 17/20 Batch 131/232: Train Loss: 0.0242\n",
      "Epoch 17/20 Batch 141/232: Train Loss: 0.0563\n",
      "Epoch 17/20 Batch 151/232: Train Loss: 0.4424\n",
      "Epoch 17/20 Batch 161/232: Train Loss: 0.0838\n",
      "Epoch 17/20 Batch 171/232: Train Loss: 0.0536\n",
      "Epoch 17/20 Batch 181/232: Train Loss: 1.5195\n",
      "Epoch 17/20 Batch 191/232: Train Loss: 0.0112\n",
      "Epoch 17/20 Batch 201/232: Train Loss: 0.0694\n",
      "Epoch 17/20 Batch 211/232: Train Loss: 0.2362\n",
      "Epoch 17/20 Batch 221/232: Train Loss: 0.0411\n",
      "Epoch 17/20 Batch 231/232: Train Loss: 0.0805\n",
      "Epoch 17/20, Train Loss: 0.1542, Train F1: 0.9740733099511725Val Loss: 1.2477, Val F1: 0.8884423503325942\n",
      "Epoch 18/20 Batch 1/232: Train Loss: 0.0081\n",
      "Epoch 18/20 Batch 11/232: Train Loss: 0.3260\n",
      "Epoch 18/20 Batch 21/232: Train Loss: 0.1089\n",
      "Epoch 18/20 Batch 31/232: Train Loss: 0.0517\n",
      "Epoch 18/20 Batch 41/232: Train Loss: 0.2493\n",
      "Epoch 18/20 Batch 51/232: Train Loss: 0.1235\n",
      "Epoch 18/20 Batch 61/232: Train Loss: 0.0503\n",
      "Epoch 18/20 Batch 71/232: Train Loss: 0.0166\n",
      "Epoch 18/20 Batch 81/232: Train Loss: 0.0181\n",
      "Epoch 18/20 Batch 91/232: Train Loss: 0.0593\n",
      "Epoch 18/20 Batch 101/232: Train Loss: 0.2092\n",
      "Epoch 18/20 Batch 111/232: Train Loss: 0.0261\n",
      "Epoch 18/20 Batch 121/232: Train Loss: 0.1712\n",
      "Epoch 18/20 Batch 131/232: Train Loss: 0.0371\n",
      "Epoch 18/20 Batch 141/232: Train Loss: 0.0697\n",
      "Epoch 18/20 Batch 151/232: Train Loss: 0.0209\n",
      "Epoch 18/20 Batch 161/232: Train Loss: 0.1634\n",
      "Epoch 18/20 Batch 171/232: Train Loss: 0.0677\n",
      "Epoch 18/20 Batch 181/232: Train Loss: 0.0113\n",
      "Epoch 18/20 Batch 191/232: Train Loss: 0.0411\n",
      "Epoch 18/20 Batch 201/232: Train Loss: 0.0385\n",
      "Epoch 18/20 Batch 211/232: Train Loss: 0.2032\n",
      "Epoch 18/20 Batch 221/232: Train Loss: 0.0330\n",
      "Epoch 18/20 Batch 231/232: Train Loss: 0.2044\n",
      "Epoch 18/20, Train Loss: 0.1348, Train F1: 0.9760398990761074Val Loss: 1.2866, Val F1: 0.8923018062003552\n",
      "Epoch 19/20 Batch 1/232: Train Loss: 0.0461\n",
      "Epoch 19/20 Batch 11/232: Train Loss: 0.0846\n",
      "Epoch 19/20 Batch 21/232: Train Loss: 0.0665\n",
      "Epoch 19/20 Batch 31/232: Train Loss: 0.1597\n",
      "Epoch 19/20 Batch 41/232: Train Loss: 0.2527\n",
      "Epoch 19/20 Batch 51/232: Train Loss: 0.0533\n",
      "Epoch 19/20 Batch 61/232: Train Loss: 0.1143\n",
      "Epoch 19/20 Batch 71/232: Train Loss: 0.0435\n",
      "Epoch 19/20 Batch 81/232: Train Loss: 0.0331\n",
      "Epoch 19/20 Batch 91/232: Train Loss: 0.0715\n",
      "Epoch 19/20 Batch 101/232: Train Loss: 0.0474\n",
      "Epoch 19/20 Batch 111/232: Train Loss: 0.0358\n",
      "Epoch 19/20 Batch 121/232: Train Loss: 0.1020\n",
      "Epoch 19/20 Batch 131/232: Train Loss: 0.1610\n",
      "Epoch 19/20 Batch 141/232: Train Loss: 0.1656\n",
      "Epoch 19/20 Batch 151/232: Train Loss: 0.3131\n",
      "Epoch 19/20 Batch 161/232: Train Loss: 0.0229\n",
      "Epoch 19/20 Batch 171/232: Train Loss: 0.0429\n",
      "Epoch 19/20 Batch 181/232: Train Loss: 0.0245\n",
      "Epoch 19/20 Batch 191/232: Train Loss: 0.0170\n",
      "Epoch 19/20 Batch 201/232: Train Loss: 0.0116\n",
      "Epoch 19/20 Batch 211/232: Train Loss: 1.1810\n",
      "Epoch 19/20 Batch 221/232: Train Loss: 0.0166\n",
      "Epoch 19/20 Batch 231/232: Train Loss: 0.0189\n",
      "Epoch 19/20, Train Loss: 0.1262, Train F1: 0.9796351812963703Val Loss: 1.3105, Val F1: 0.8846723646723647\n",
      "Epoch 20/20 Batch 1/232: Train Loss: 1.2853\n",
      "Epoch 20/20 Batch 11/232: Train Loss: 0.0690\n",
      "Epoch 20/20 Batch 21/232: Train Loss: 0.0447\n",
      "Epoch 20/20 Batch 31/232: Train Loss: 0.0382\n",
      "Epoch 20/20 Batch 41/232: Train Loss: 0.0248\n",
      "Epoch 20/20 Batch 51/232: Train Loss: 0.0470\n",
      "Epoch 20/20 Batch 61/232: Train Loss: 0.0744\n",
      "Epoch 20/20 Batch 71/232: Train Loss: 0.0049\n",
      "Epoch 20/20 Batch 81/232: Train Loss: 0.3178\n",
      "Epoch 20/20 Batch 91/232: Train Loss: 0.0484\n",
      "Epoch 20/20 Batch 101/232: Train Loss: 0.0491\n",
      "Epoch 20/20 Batch 111/232: Train Loss: 0.1037\n",
      "Epoch 20/20 Batch 121/232: Train Loss: 0.0132\n",
      "Epoch 20/20 Batch 131/232: Train Loss: 0.0252\n",
      "Epoch 20/20 Batch 141/232: Train Loss: 0.8676\n",
      "Epoch 20/20 Batch 151/232: Train Loss: 0.0376\n",
      "Epoch 20/20 Batch 161/232: Train Loss: 0.0312\n",
      "Epoch 20/20 Batch 171/232: Train Loss: 0.0714\n",
      "Epoch 20/20 Batch 181/232: Train Loss: 0.0512\n",
      "Epoch 20/20 Batch 191/232: Train Loss: 0.0137\n",
      "Epoch 20/20 Batch 201/232: Train Loss: 0.0220\n",
      "Epoch 20/20 Batch 211/232: Train Loss: 0.0440\n",
      "Epoch 20/20 Batch 221/232: Train Loss: 0.1149\n",
      "Epoch 20/20 Batch 231/232: Train Loss: 0.0724\n",
      "Epoch 20/20, Train Loss: 0.1238, Train F1: 0.9806537413645611Val Loss: 1.3175, Val F1: 0.8817946653418532\n",
      "Last model saved to last_model_epoch20.pth\n",
      "Best model loaded from best_model_epoch13_valf1_0.8981.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134304/1668301912.py:101: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "05c54f41",
   "metadata": {},
   "source": [
    "# Load and evaluate the best finetuned checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "id": "920d8f3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T14:46:40.069877Z",
     "start_time": "2025-06-05T14:46:39.035334Z"
    }
   },
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "# Load for best model for testing\n",
    "model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "print(f\"Best model loaded from {best_model_path}\")\n",
    "\n",
    "model.eval()\n",
    "test_loss_meter = Meter()\n",
    "all_test_predictions = []\n",
    "all_test_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        \n",
    "        outputs = model(inputs, channels, task='cls').squeeze()\n",
    "        \n",
    "        if multilabel:\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "        else:\n",
    "            outputs = torch.softmax(outputs, dim=-1)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss_meter.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        if not multilabel:\n",
    "            outputs = torch.softmax(outputs, dim=-1)\n",
    "\n",
    "        for i in range(len(outputs)):\n",
    "            all_test_predictions.append(outputs[i].cpu().detach().numpy())\n",
    "            all_test_targets.append(targets[i].cpu().detach().numpy())\n",
    "\n",
    "test_loss = test_loss_meter.average()\n",
    "test_predictions = np.array(all_test_predictions)\n",
    "test_targets = np.array(all_test_targets)\n",
    "test_f1_per_label = calculate_f1_per_label(test_predictions, test_targets, multilabel=multilabel)\n",
    "test_f1_macro = np.mean(test_f1_per_label)\n",
    "\n",
    "test_auc = calculate_auc(test_predictions, test_targets, multilabel=multilabel)\n",
    "test_aupr = calculate_aupr(test_predictions, test_targets, multilabel=multilabel)\n",
    "\n",
    "\n",
    "# Recall \n",
    "if multilabel:\n",
    "    test_recall = recall_score(test_targets, (test_predictions > 0.5).astype(int), average='macro')\n",
    "else:\n",
    "    test_recall = recall_score(test_targets, np.argmax(test_predictions, axis=1), average='macro')\n",
    "\n",
    "# Precision (for multilabel or multiclass)\n",
    "if multilabel:\n",
    "    test_precision = precision_score(test_targets, (test_predictions > 0.5).astype(int), average='macro')\n",
    "else:\n",
    "    test_precision = precision_score(test_targets, np.argmax(test_predictions, axis=1), average='macro')\n",
    "\n",
    "# Accuracy\n",
    "if multilabel:\n",
    "    test_accuracy = accuracy_score(test_targets, (test_predictions > 0.5).astype(int))\n",
    "else:\n",
    "    test_accuracy = accuracy_score(test_targets, np.argmax(test_predictions, axis=1))\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test F1 (macro): {test_f1_macro:.4f}, Test AUC: {test_auc:.4f}, Test AUPR: {test_aupr:.4f}, '\n",
    "      f'Test Recall: {test_recall:.4f}, Test Precision: {test_precision:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model loaded from best_model_epoch13_valf1_0.8981.pth\n",
      "Test Loss: 0.7863, Test F1 (macro): 0.8830, Test AUC: 0.9664, Test AUPR: 0.9106, Test Recall: 0.9067, Test Precision: 0.8671, Test Accuracy: 0.9025\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
